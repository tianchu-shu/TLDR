{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrSBcbpabAxJ"
   },
   "source": [
    "#Load a sample article from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/tianchushu/Library/Caches/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472/bs4-0.0.1-cp37-none-any.whl\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>=1.2\n",
      "  Downloading soupsieve-1.9.5-py2.py3-none-any.whl (33 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.8.2 bs4-0.0.1 soupsieve-1.9.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTJ8P5vxbCK2"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "cOITcVLnh_cj",
    "outputId": "1650960c-691e-4bb5-97bf-1c84aad49619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\tDecember 17, 2019\t\t\tThe University of Illinois at Chicago’s College of Liberal Arts and Sciences has received $3 million from The Davee Foundation to assist high-achieving English and history undergraduate students who have prohibitive levels of financial need.The new Adeline Barry Davee and Ruth Dunbar Davee Scholarship Fund represents the academic interests of the benefactors, continues the foundation’s legacy of support at UIC, and marks its final endowment grant. The 55-year-old organization, which has donated roughly $176 million since the mid-1990s, closes down at the end of December. With the new endowment, the College of Liberal Arts and Sciences has received a total of $6 million from the foundation in aid for high-achieving students with high levels of financial need since 2006. The Davee Scholarship Fund, which is the college’s largest scholarship endowment, has covered the full cost of attendance for more than 23 academically gifted students at UIC.“The Davee Foundation’s history of generous support has provided\n",
      "countless opportunities for students in need of financial assistance,” said UIC\n",
      "Chancellor Michael Amiridis. “We are deeply appreciative of this gift that will\n",
      "continue to open doors to a liberal arts education for our students.”“With this endowment, talented students in English and history will be able to pursue their academic passions free of financial hardship,” said Astrida Orle Tantillo, dean of the college. “We are truly honored to have been selected to receive the foundation’s final act of generosity and to carry forward the legacy of two remarkable Davee women. It has been a tremendous partnership, and we are grateful for The Davee Foundation’s long-standing support of current and future students studying in the liberal arts and sciences.”  The Davee Foundation’s gift supports IGNITE: The Campaign for UIC, through which UIC and the College of Liberal Arts and Sciences will redefine the model for student experience and success.  After earning a master’s\n",
      "degree in history, Adeline Barry Davee’s career involved raising money for\n",
      "scholarships at her alma mater Flora Stone Mather College before exploring her\n",
      "interest in operational and historical records through archivist positions with\n",
      "the Library of Congress, the National Archives and the U.S. Navy. She married businessman\n",
      "Ken Davee shortly after World War II and joined him at his market research firm\n",
      "Davee, Koehnlein and Keating Company as vice president. In 1964 the couple founded The Davee Foundation, which demonstrated a Chicago-centric philanthropic commitment in its support of a wide variety of cultural, scientific, public affairs and educational entities. Adeline remained on the foundation’s board until she passed away in 1987. Ruth Dunbar Davee, who married Ken in 1988, graduated from the University of Illinois at Urbana-Champaign in 1935 with a bachelor’s degree in English. With a Ph.D. from Northwestern University, she went on to become a professor at several universities and a journalist at the Chicago Sun-Times, where she originated the position of education editor. As a member of The Davee\n",
      "Foundation’s board, she oversaw major philanthropic efforts in the Chicago area\n",
      "and particularly those in support of educational causes, including UIC’s\n",
      "mission of access and opportunity in higher education. Ruth Dunbar Davee served\n",
      "as president of The Davee Foundation upon the death of Ken in 1998 and led the\n",
      "organization until her death in 2011. “In its final gift, The Davee Foundation wished to honor both Adeline Barry Davee and Ruth Dunbar Davee. We could think of no better way to honor them than by having a scholarship in both their names, in their respective disciplines, at the University of Illinois at Chicago. We have been very pleased by the quality of students at UIC that have received scholarships from the previously existing The Davee Scholarship Fund, and look forward to the same quality of students going forward with these new scholarships,” said James W. Dugdale Jr., president of The Davee Foundation.In each academic year, the endowment calls for at least two Chicago area students, one from English and one from history, be awarded the scholarship, which willensure the recipients’ cost of attendance is fully covered. The scholarship is renewable from entry through graduation for up to five years for recipients who must enter UIC with, and maintain, at least a 3.0 GPA. The first Adeline Barry Davee and Ruth Dunbar Davee\n",
      "Scholars, along with the latest students to earn support via The Davee\n",
      "Scholarship Fund will be selected next year and celebrated during the college’s\n",
      "annual scholarship luncheon in March.\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t312-996-7681\n",
      "\n",
      "\n",
      "\n",
      "Categories\n",
      " Topics, , , , \n"
     ]
    }
   ],
   "source": [
    "url = 'https://today.uic.edu/3-million-gift-from-the-davee-foundation-to-support-english-history-scholarships-at-uic'\n",
    "res = requests.get(url)\n",
    "html_page = res.content\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "text = soup.find_all(text=True)\n",
    "\n",
    "output = ''\n",
    "\n",
    "for t in text:\n",
    "\tif t.parent.name == 'p':\n",
    "\t\toutput += '{}'.format(t)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6Tn0fDZriaI1",
    "outputId": "99c07721-7711-45ce-b462-a70881ee6038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$3 million gift from The Davee Foundation to support English, history scholarships at UIC | UIC Today\n"
     ]
    }
   ],
   "source": [
    "title = soup.title\n",
    "titleText = title.get_text()\n",
    "print(titleText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "j7-W2XPIEN43",
    "outputId": "3e454740-3bcc-4d84-8572-92ed87fa3605"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tianchushu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenzie the string into sentences\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "#raw = tokenize.sent_tokenize(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AjiCGyYsLF9v",
    "outputId": "185d43ec-ba85-4a4a-a97d-a3b212a6e4d1"
   },
   "outputs": [],
   "source": [
    "#listToStr = ' '.join([str(elem) for elem in clean]) \n",
    "#print(listToStr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "SDZNpPnq1Jj1",
    "outputId": "69b6b001-32b6-4c20-a195-38ff81948e61"
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "result =summarize(output, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M4Y0RrjDB-fA"
   },
   "outputs": [],
   "source": [
    "#cleaning the result text\n",
    "import re\n",
    "clean =[]\n",
    "for s in result:\n",
    "    s = s.replace('\\n', ' ').replace('\\t', ' ').replace(\"'\",\"\").replace('\"','')\n",
    "    clean.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 17, 2019\t\t\tThe University of Illinois at Chicago’s College of Liberal Arts and Sciences has received $3 million from The Davee Foundation to assist high-achieving English and history undergraduate students who have prohibitive levels of financial need.The new Adeline Barry Davee and Ruth Dunbar Davee Scholarship Fund represents the academic interests of the benefactors, continues the foundation’s legacy of support at UIC, and marks its final endowment grant.\n",
      "The Davee Scholarship Fund, which is the college’s largest scholarship endowment, has covered the full cost of attendance for more than 23 academically gifted students at UIC.“The Davee Foundation’s history of generous support has provided\n",
      "Dugdale Jr., president of The Davee Foundation.In each academic year, the endowment calls for at least two Chicago area students, one from English and one from history, be awarded the scholarship, which willensure the recipients’ cost of attendance is fully covered.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(output, word_count=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-ArrAFB-7PUG",
    "outputId": "3d9b451b-3c73-41b2-b296-8115b0ca5630"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tianchushu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPCyX-Pp8v8Q"
   },
   "outputs": [],
   "source": [
    "links = {\n",
    "    'webpage-1': set(['webpage-2', 'webpage-4', 'webpage-5', 'webpage-6', 'webpage-8', 'webpage-9', 'webpage-10']),\n",
    "    'webpage-2': set(['webpage-5', 'webpage-6']),\n",
    "    'webpage-3': set(['webpage-10']),\n",
    "    'webpage-4': set(['webpage-9']),\n",
    "    'webpage-5': set(['webpage-2', 'webpage-4']),\n",
    "    'webpage-6': set([]), # dangling page\n",
    "    'webpage-7': set(['webpage-1', 'webpage-3', 'webpage-4']),\n",
    "    'webpage-8': set(['webpage-1']),\n",
    "    'webpage-9': set(['webpage-1', 'webpage-2', 'webpage-3', 'webpage-8', 'webpage-10']),\n",
    "    'webpage-10': set(['webpage-2', 'webpage-3', 'webpage-8', 'webpage-9']),\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "0N9kMqiY8yq4",
    "outputId": "3c838bd1-0816-40be-b3f4-39d8dc53ef88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'webpage-1': 0, 'webpage-2': 1, 'webpage-3': 2, 'webpage-4': 3, 'webpage-5': 4, 'webpage-6': 5, 'webpage-7': 6, 'webpage-8': 7, 'webpage-9': 8, 'webpage-10': 9}\n"
     ]
    }
   ],
   "source": [
    "def build_index(links):\n",
    "    website_list = links.keys()\n",
    "    return {website: index for index, website in enumerate(website_list)}\n",
    " \n",
    "website_index = build_index(links)\n",
    "print(website_index)\n",
    "# {'webpage-10': 3, 'webpage-9': 0, 'webpage-8': 1, 'webpage-1': 2, 'webpage-3': 4, 'webpage-2': 5, 'webpage-5': 6, 'webpage-4': 7, 'webpage-7': 8, 'webpage-6': 9}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "V5iS_VdL8oBH",
    "outputId": "436d536b-7fb1-48d7-80c2-2d071c8abfb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.14285714 0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.14285714 0.14285714 0.14285714]\n",
      " [0.         0.         0.         0.         0.5        0.5\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.        ]\n",
      " [0.         0.5        0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.1        0.1        0.1        0.1        0.1        0.1\n",
      "  0.1        0.1        0.1        0.1       ]\n",
      " [0.33333333 0.         0.33333333 0.33333333 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.2        0.2        0.2        0.         0.         0.\n",
      "  0.         0.2        0.         0.2       ]\n",
      " [0.         0.25       0.25       0.         0.         0.\n",
      "  0.         0.25       0.25       0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "def build_transition_matrix(links, index):\n",
    "    total_links = 0\n",
    "    A = np.zeros((len(index), len(index)))\n",
    "    for webpage in links:\n",
    "        # dangling page\n",
    "        if not links[webpage]:\n",
    "            # Assign equal probabilities to transition to all the other pages\n",
    "            A[index[webpage]] = np.ones(len(index)) / len(index)\n",
    "        else:\n",
    "            for dest_webpage in links[webpage]:\n",
    "                total_links += 1\n",
    "                A[index[webpage]][index[dest_webpage]] = 1.0 / len(links[webpage])\n",
    " \n",
    "    return A\n",
    " \n",
    "A = build_transition_matrix(links, website_index)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NYWNdi698i6n",
    "outputId": "6f0216ab-12f6-435a-8364-5e1836add51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [0.1300934  0.1305122  0.08116465 0.085402   0.09427366 0.09427366\n",
      " 0.02301397 0.09044235 0.13933698 0.13148714]\n",
      "1.0\n",
      "[8, 9, 1, 0, 4, 5, 7, 3, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "def pagerank(A, eps=0.0001, d=0.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
    "        delta = abs(new_P - P).sum()\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P\n",
    " \n",
    "results = pagerank(A)\n",
    " \n",
    "print(\"Results:\", results) # [ 0.13933698,  0.09044235,  0.1300934 ,  0.13148714,  0.08116465, 0.1305122 ,  0.09427366,  0.085402  ,  0.02301397,  0.09427366]\n",
    "print(sum(results)) # 1.0\n",
    "print([item[0] for item in sorted(enumerate(results), key=lambda item: -item[1])]) # [0, 3, 5, 2, 6, 9, 1, 7, 4, 8]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A841Phrt8a2R"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-58e916887fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentence_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_ranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'S' is not defined"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter \n",
    " \n",
    "sentence_ranks = pagerank(S)\n",
    " \n",
    "print(sentence_ranks)\n",
    " \n",
    "# Get the sentences ordered by rank\n",
    "ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "print(ranked_sentence_indexes)\n",
    " \n",
    "# Suppose we want the 5 most import sentences\n",
    "SUMMARY_SIZE = 5\n",
    "SELECTED_SENTENCES = sorted(ranked_sentence_indexes[:SUMMARY_SIZE])\n",
    "print(SELECTED_SENTENCES)\n",
    " \n",
    "# Fetch the most important sentences\n",
    "summary = itemgetter(*SELECTED_SENTENCES)(sentences)\n",
    " \n",
    "# Print the actual summary\n",
    "for sentence in summary:\n",
    "    print(' '.join(sentence))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "EGwNHi4-75sV",
    "outputId": "8c24ed4d-7c62-4c04-9523-dac0d5048e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999998\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f0b0d0d9c5bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# One out of 2 non-stop words differ => 0.5 similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a good sentence\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This is a bad sentence\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.cluster.util import cosine_distance\n",
    " \n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "# One out of 5 words differ => 0.8 similarity\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))\n",
    " \n",
    "# One out of 2 non-stop words differ => 0.5 similarity\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split(), stopwords.words('english')))\n",
    " \n",
    "# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a good sentence\".split(), stopwords.words('english')))\n",
    " \n",
    "# Completely different sentences=> 0.0\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"I want to go to the market\".split(), stopwords.words('english')))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zk4wM_Cz-HO5",
    "outputId": "30bbfccf-3d02-4ec0-872d-83bd3cc6bb4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.tagged.TaggedCorpusView"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "U9lcxlND7nni",
    "outputId": "2ae74398-c0af-4c88-8d91-dc022a546d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\tFebruary 4, 2020\t\t\tUIC Dining Services is bringing the farm to the dining hall with new, eco-friendly Farmshelfs throughout campus. The indoor, bookcase-size farms grow organic, fresh herbs and greens that UIC students can enjoy in their meals in campus dining halls. The six units are currently housed in the Stukel Towers dining hall and the East Terrace dining hall in SCE. They also are used for catered events on campus.Farmshelf also brings sustainable benefits to campus. Since the plants are grown in-house, there is no need to burn fuel in transportation, lessening UIC’s carbon footprint. UIC also is minimizing the excess use of water on vegetation by 90%, as well as reducing packaging waste. The Farmshelf uses high-tech, energy-efficient LED lights that not only improve taste, but also maximize the nutrient density. “The cooks love the new Farmshelfs and can cut crop such as arugula straight from the shelf,” said Sean Gordon, senior director of dining. “You can’t get any fresher than that.” \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categories\n",
      " \n",
      "1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nltk/cluster/util.py:133: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('brown')\n",
    " \n",
    " \n",
    "# Get a text from the Brown Corpus\n",
    "sentences = output\n",
    " \n",
    "print(sentences)\n",
    "# [[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.'], [u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.'], ...]\n",
    " \n",
    "print(len(sentences))  #  98\n",
    " \n",
    "# get the english list of stopwords\n",
    "stop_words =nltk.corpus.stopwords.words('english')\n",
    " \n",
    " \n",
    "def build_similarity_matrix(sentences, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S\n",
    " \n",
    "S = build_similarity_matrix(sentences, stop_words)    \n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKePt8ZC6rao"
   },
   "outputs": [],
   "source": [
    "def textrank(sentences, top_n=5, stopwords=None):\n",
    "    \"\"\"\n",
    "    sentences = a list of sentences [[w11, w12, ...], [w21, w22, ...], ...]\n",
    "    top_n = how may sentences the summary should contain\n",
    "    stopwords = a list of stopwords\n",
    "    \"\"\"\n",
    "    S = build_similarity_matrix(sentences, stop_words) \n",
    "    sentence_ranks = pagerank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(sentences)\n",
    "    return summary\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "kMGbBYF57KC8",
    "outputId": "1dc38ef9-ed96-4301-a5b4-89d5382d0992"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textrank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-e30af5a55413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s. %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'textrank' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(textrank(sentences, stopwords=stopwords.words('english'))):\n",
    "    print(\"%s. %s\" % ((idx + 1), ' '.join(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjz3qzEF-Ndb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TLDR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
